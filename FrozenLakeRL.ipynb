{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "5A_Reinforcement_Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnhshl/cc-ai/blob/main/FrozenLakeRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj8sw5EMxtpF"
      },
      "source": [
        "# Frozen Lake - Einfaches Q-Learning Beispiel\n",
        "\n",
        "Wir verwenden eine von openAI zur Verfügung gestellte Simulationsumgebung für Reinforcement Probleme (https://gym.openai.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2FtIojzxtpN"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhM5zqsOxtpP"
      },
      "source": [
        "### FrozenLake (a Grid World)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zumHRBp5xtpP"
      },
      "source": [
        "Wir nutzen ein einfaches Beispiel: FrozenLake.\n",
        "Problembeschreibung von OpenAI:\n",
        "\n",
        "> *Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.*\n",
        "\n",
        "\n",
        "<img src=\"https://zitaoshen.rbind.io/project/rl/1-min-of-reinforcement-learning-q-learning/featured.png?raw=1\" alt=\"FrozenLake\" style=\"width: 400px;\"/>\n",
        "\n",
        "Wir starten in der linken oberen Ecke (S). Unser Ziel ist es, zur unteren rechten Ecke (G) zu gelangen, ohne zwischendurch in ein Loch (H) zu fallen.\n",
        "\n",
        "Es gibt vier mögliche Aktionen:\n",
        "\n",
        "0=links, 1=runter, 2=rechts, 3=rauf\n",
        "\n",
        "Die Stati sind die 16 Felder (0 .. 15).\n",
        "\n",
        "\n",
        "Jede Bewegung führt dazu, dass sich der Status des Agenten von $s_t$ zu $s_{t+1}$ ändert, wenn er seinen Standort ändert, es sei denn, er versucht, sich in Richtung einer Wand zu bewegen, was dazu führt, dass sich der Status des Agenten nicht ändert (der Agent bewegt sich nicht).\n",
        "Für das Erreichen des Ziels (G) erhalten wir eine positive Belohnung von „+1“, die je nach Dauer abgezinst wird.\n",
        "Obwohl es keine negative Belohnung für das Fallen in ein Loch gibt (H), zahlt der Agent dennoch eine Strafe in dem Sinne, dass das Fallen in das Loch die Episode beendet und ihn daher daran hindert, eine Belohnung zu erhalten.\n",
        "Wir wollen eine Richtlinie $\\pi$ lernen, die uns in möglichst wenigen Schritten von unserem Startort (S) zum Ziel (G) führt.\n",
        "\n",
        "Das Problem ist schwieriger, als es sich für uns in der ersten Betrachtung darstellt:\n",
        "\n",
        "- **Kenntnis der Zustände und Übergangswahrscheinlichkeiten:** Aus der globalen Sicht von oben könnte der erste Gedanke sein, einen Weg vom Start bis zum Ziel zu planen, genau wie bei einem Labyrinth.\n",
        "Diese Ansicht wird uns jedoch den Algorithmus-Designern zur Verfügung gestellt, damit wir das vorliegende Problem visualisieren können.\n",
        "Der Agent, der die Aufgabe lernt, erhält dieses Vorwissen *nicht*; Alles was wir sagen werden ist, dass es 16 Stati und 4 mögliche Aktionen in jedem Status gibt.\n",
        "Eine passendere Analogie wäre:  Sie stehen mit verbundenen Augen auf einem zugeforenen See. Jedes mal, wenn Sie sich entscheiden, einen Schritt in eine von vier Richtungen zu machen, wird Ihnen Ihr neuer Zustand (Standort) mitgeteilt. Finden Sie das Frisbee, ohne ins Eis einzubrechen, erhalten Sie eine Belohnung, die umso größer ist, je schneller Sie das Frisbee finden.\n",
        "\n",
        "- **Kenntnis des Ziels (Belohnung):** \n",
        "Der Agent weiß  *nicht*, was das Ziel ist. \n",
        "Vielmehr lernen Sie das Ziel, indem es Belohnungen (oder Strafen) gibt, und der Algorithmus aktualisiert seine Richtlinie zur Wahl von Aktionen so, dass er Aktionen mit größerer Wahrscheinlichkeit erneut durchführt, die voraussichtlich zu einer späteren Belohnung führen. \n",
        "Beachten Sie, dass dies bedeutet, dass ein Agent, wenn er bestimmte Belohnungen nie erhält, nicht weiß, dass sie existieren.\n",
        "\n",
        "- **Vorkenntnisse in Pfadfindung, Physik, etc.:** Als Mensch bringen Sie, auch wenn Sie diese Aufgabe noch nicht gelöst haben, enorme Vorkenntnisse zu diesem Problem mit.\n",
        "Sie wissen beispielsweise, dass der kürzeste Weg zu einem Ziel eine Linie ist.\n",
        "Sie wissen, dass Norden, Süden, Osten und Westen Richtungen sind und dass Sie nach Norden und dann nach Süden zurückkehren, wo Sie bereits waren.\n",
        "Sie wissen, dass Eis rutschig ist.\n",
        "Sie wissen, dass eisiges Wasser kalt ist.\n",
        "Sie wissen, dass es schlecht ist, in eiskaltem Wasser zu sein.\n",
        "Der Agent weiß von all diesen Dingen nichts; seine anfängliche Richtlinie besteht im Wesentlichen darin, Aktionen vollständig zufällig auszuwählen.\n",
        "Am Ende des Trainings wird es immer noch nicht wissen, was abstrakte Konzepte wie \"Nord/Süd\", \"kalt\" oder \"rutschig\" bedeuten, aber es wird (hoffentlich) eine gute Politik gelernt haben, die es ihm ermöglicht, das Ziel zu erreichen.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqtloydzxtpQ"
      },
      "source": [
        "env1 = gym.make('FrozenLake-v0')\n",
        "env=env1\n",
        "print(env.observation_space, env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKNMdEtextpS"
      },
      "source": [
        "Über einen `reset() `kann das Environment jeweils \"auf Start\" zurückgesetzt werden. Der Status $s$ wird dann also 0 (das erste Feld) sein.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2MH2fKWxtpS"
      },
      "source": [
        "s = env.reset()\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8iWXfrExtpT"
      },
      "source": [
        "Mittels `render()` kann der aktuelle Status dargestellt werden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktoBL6iHxtpT"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEzQbKCxxtpT"
      },
      "source": [
        "Die Standard FrozenLake-Umgebung ist \"rutschig\". Wegen des Eises haben Sie, wenn Sie versuchen, in eine Richtung zu gehen, eine 1/3-Chance, in die von Ihnen gemeinte Richtung und die beiden angrenzenden Richtungen zu gehen. Wenn wir zum Beispiel versuchen, nach rechts zu gehen, haben wir die gleiche Wahrscheinlichkeit, stattdessen abzurutschen und nach oben und unten zu gehen. Dies macht die Sache etwas komplizierter.\n",
        "\n",
        "Wir definieren uns zunächst eine \"nicht rutschige\" Umgebung, in der die Aktion deterministisch ausgeführt wird.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeIRKfQQxtpU"
      },
      "source": [
        "# Non-slippery version\n",
        "\n",
        "from gym.envs.registration import register\n",
        "register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNM9rjy1OFl7"
      },
      "source": [
        "env2 = gym.make('FrozenLakeNotSlippery-v0')\n",
        "env=env2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBmHMsPQxtpU"
      },
      "source": [
        "Mittels `step(action)` führen wir einen Schritt durch.\n",
        "`step(2)` entspricht also einer Bewegung nach rechts.\n",
        "Beachten Sie, dass die Ausgabe ein Tupel von vier Elementen ist: der nächste Statusg (`object`), der reward (`float`), ob die Episode fertig ist (`boolean`) und weitere Infos (`dict` ), die für das Debuggen nützlich sein können."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xux3PZInxtpV"
      },
      "source": [
        "env.reset()\n",
        "env.render()\n",
        "env.step(2)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COXHdmzgxtpX"
      },
      "source": [
        "Sobald wir in einem Loch (H) oder im Ziel (G) gelandet sind, ist die Episode beendet. Weitere Aktionen können dann nicht mehr durchgeführt werden.\n",
        "\n",
        "Wir können zunächst Aktionen einfach einmal zufällig auswählen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wbEHr1RxtpX"
      },
      "source": [
        "env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = env.action_space.sample()\n",
        "    _, _, done, _ = env.step(action)\n",
        "env.render()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fcbe6FuxtpY"
      },
      "source": [
        "\n",
        "\n",
        "### Q-learning in FrozenLake\n",
        "FrozenLake ist eine sehr einfache Problemstellung.\n",
        "\n",
        "Mit nur 16 Zuständen und 4 Aktionen sind nur 64 Zustands-Aktions-Paare möglich. Unsere Q-Tabelle ist alsa 16x4. Wir initialisieren die Q Tabelle mit Nullen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAuxrXQoxtpY"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#Initialize table with all zeros to be uniform\n",
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "print(Q)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsLlVkBgxtpY"
      },
      "source": [
        "#### Trainingsparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9MRVzCLxtpZ"
      },
      "source": [
        "# Learning parameters\n",
        "alpha = 0.1\n",
        "gamma = 0.95\n",
        "num_episodes = 2000\n",
        "\n",
        "# array of reward for each episode\n",
        "rs = np.zeros([num_episodes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95kZBLorxtpZ"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGrTTDxnxtpZ"
      },
      "source": [
        "for i in range(num_episodes):\n",
        "    # Set total reward and time to zero, done to False\n",
        "    r_sum_i = 0\n",
        "    t = 0\n",
        "    done = False\n",
        "    \n",
        "    #Reset environment and get first new observation\n",
        "    s = env.reset()\n",
        "    \n",
        "    while not done:\n",
        "        # Choose an action by greedily (with noise) from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i/10+1)))\n",
        "        \n",
        "        # Get new state and reward from environment\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        \n",
        "        # Update Q-Table with new knowledge\n",
        "        Q[s,a] = (1 - alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:]))\n",
        "        \n",
        "        # Add reward to episode total\n",
        "        r_sum_i += r*gamma**t\n",
        "        \n",
        "        # Update state and time\n",
        "        s = s1\n",
        "        t += 1\n",
        "    rs[i] = r_sum_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGdEKkVoxtpZ"
      },
      "source": [
        "Wie entwickelt sich der Reward im Verlaufe der Episoden?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbopkBISxtpZ"
      },
      "source": [
        "## Plot reward vs episodes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sliding window average\n",
        "r_cumsum = np.cumsum(np.insert(rs, 0, 0)) \n",
        "r_cumsum = (r_cumsum[50:] - r_cumsum[:-50]) / 50\n",
        "\n",
        "# Plot\n",
        "plt.plot(r_cumsum)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s7OIdt3xtpa"
      },
      "source": [
        "Wie oft hat unser Agent das Ziel tatsächlich erreicht?\n",
        "Dies erklärt nicht, wie schnell der Agent dort ankam (was auch von Interesse sein könnte), aber wir ignorieren dass.\n",
        "Wir teilen die 2000 Episoden in 10 Buckets a 200 und schauen, wie oft jeweils das Ziel erreicht wurde."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nntSJxAHxtpa"
      },
      "source": [
        "# Print number of times the goal was reached\n",
        "N = len(rs)//10\n",
        "num_Gs = np.zeros(10)\n",
        "\n",
        "for i in range(10):\n",
        "    num_Gs[i] = np.sum(rs[i*N:(i+1)*N] > 0)\n",
        "    \n",
        "print(\"Rewards: {0}\".format(num_Gs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPhTQndidltI"
      },
      "source": [
        "Welche (deterministische) Route nimmt unser Agent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c4znjcP4cLZ"
      },
      "source": [
        "def perform_and_render():\n",
        "  \n",
        "  s = env.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    print(env.nS)\n",
        "    env.render()\n",
        "    action = np.argmax(Q[s,:]) \n",
        "    print(action)\n",
        "    s, _, done, _ = env.step(action)\n",
        "  env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkEUXxTy5LqE"
      },
      "source": [
        "perform_and_render()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAko4mdhxtpa"
      },
      "source": [
        "Nutzen wir nun die Originalumgebung mit der Wahrscheinlichkeit, \"auszurutschen\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6UrQZwP8iVu"
      },
      "source": [
        "env = env1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mUzNS24d8gh"
      },
      "source": [
        "Nachdem wir jetzt das Environment zurückgesetzt haben, können wir in dieser Umgebung trainieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciSJHTd9xtpa"
      },
      "source": [
        "\n",
        "#Initialize table with all zeros to be uniform\n",
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Learning parameters\n",
        "alpha = 0.1\n",
        "gamma = 0.95\n",
        "num_episodes = 2000\n",
        "\n",
        "# array of reward for each episode\n",
        "rs = np.zeros([num_episodes])\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    # Set total reward and time to zero, done to False\n",
        "    r_sum_i = 0\n",
        "    t = 0\n",
        "    done = False\n",
        "    \n",
        "    #Reset environment and get first new observation\n",
        "    s = env.reset()\n",
        "    \n",
        "    while not done:\n",
        "        # Choose an action by greedily (with noise) from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i/10+1)))\n",
        "        \n",
        "        # Get new state and reward from environment\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        \n",
        "        # Update Q-Table with new knowledge\n",
        "        Q[s,a] = (1 - alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:]))\n",
        "        \n",
        "        # Add reward to episode total\n",
        "        r_sum_i += r*gamma**t\n",
        "        \n",
        "        # Update state and time\n",
        "        s = s1\n",
        "        t += 1\n",
        "    rs[i] = r_sum_i\n",
        "\n",
        "## Plot reward vs episodes\n",
        "# Sliding window average\n",
        "r_cumsum = np.cumsum(np.insert(rs, 0, 0)) \n",
        "r_cumsum = (r_cumsum[50:] - r_cumsum[:-50]) / 50\n",
        "\n",
        "# Plot\n",
        "plt.plot(r_cumsum)\n",
        "plt.show()\n",
        "\n",
        "# Print number of times the goal was reached\n",
        "N = len(rs)//10\n",
        "num_Gs = np.zeros(10)\n",
        "\n",
        "for i in range(10):\n",
        "    num_Gs[i] = np.sum(rs[i*N:(i+1)*N] > 0)\n",
        "    \n",
        "print(\"Rewards: {0}\".format(num_Gs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5V-Qq-6AbDo"
      },
      "source": [
        "perform_and_render()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}