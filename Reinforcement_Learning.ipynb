{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "5A_Reinforcement_Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnhshl/cc-ai/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj8sw5EMxtpF"
      },
      "source": [
        "# Frozen Lake - Einfaches Q-Learning Beispiel\n",
        "\n",
        "Wir verwenden eine von openAI zur Verfügung gestellte Simulationsumgebung für Reinforcement Probleme (https://gym.openai.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2FtIojzxtpN"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhM5zqsOxtpP"
      },
      "source": [
        "### FrozenLake (a Grid World)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zumHRBp5xtpP"
      },
      "source": [
        "Wir nutzen ein einfaches Beispiel: FrozenLake.\n",
        "Problembeschreibung von OpenAI:\n",
        "\n",
        "> *Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.*\n",
        "\n",
        "\n",
        "<img src=\"https://zitaoshen.rbind.io/project/rl/1-min-of-reinforcement-learning-q-learning/featured.png?raw=1\" alt=\"FrozenLake\" style=\"width: 400px;\"/>\n",
        "\n",
        "Wir starten in der linken oberen Ecke (S). Unser Ziel ist es, zur unteren rechten Ecke (G) zu gelangen, ohne zwischendurch in ein Loch (H) zu fallen.\n",
        "\n",
        "Es gibt vier mögliche Aktionen:\n",
        "\n",
        "0=links, 1=runter, 2=rechts, 3=rauf\n",
        "\n",
        "Die Stati sind die 16 Felder (0 .. 15).\n",
        "\n",
        "\n",
        "Jede Bewegung führt dazu, dass sich der Status des Agenten von $s_t$ zu $s_{t+1}$ ändert, wenn er seinen Standort ändert, es sei denn, er versucht, sich in Richtung einer Wand zu bewegen, was dazu führt, dass sich der Status des Agenten nicht ändert (der Agent bewegt sich nicht).\n",
        "Für das Erreichen des Ziels (G) erhalten wir eine positive Belohnung von „+1“, die je nach Dauer abgezinst wird.\n",
        "Obwohl es keine negative Belohnung für das Fallen in ein Loch gibt (H), zahlt der Agent dennoch eine Strafe in dem Sinne, dass das Fallen in das Loch die Episode beendet und ihn daher daran hindert, eine Belohnung zu erhalten.\n",
        "Wir wollen eine Richtlinie $\\pi$ lernen, die uns in möglichst wenigen Schritten von unserem Startort (S) zum Ziel (G) führt.\n",
        "\n",
        "Das Problem ist schwieriger, als es sich für uns in der ersten Betrachtung darstellt:\n",
        "\n",
        "- **Kenntnis der Zustände und Übergangswahrscheinlichkeiten:** Aus der globalen Sicht von oben könnte der erste Gedanke sein, einen Weg vom Start bis zum Ziel zu planen, genau wie bei einem Labyrinth.\n",
        "Diese Ansicht wird uns jedoch den Algorithmus-Designern zur Verfügung gestellt, damit wir das vorliegende Problem visualisieren können.\n",
        "Der Agent, der die Aufgabe lernt, erhält dieses Vorwissen *nicht*; Alles was wir sagen werden ist, dass es 16 Stati und 4 mögliche Aktionen in jedem Status gibt.\n",
        "Eine passendere Analogie wäre:  Sie stehen mit verbundenen Augen auf einem zugeforenen See. Jedes mal, wenn Sie sich entscheiden, einen Schritt in eine von vier Richtungen zu machen, wird Ihnen Ihr neuer Zustand (Standort) mitgeteilt. Finden Sie das Frisbee, ohne ins Eis einzubrechen, erhalten Sie eine Belohnung, die umso größer ist, je schneller Sie das Frisbee finden.\n",
        "\n",
        "- **Kenntnis des Ziels (Belohnung):** \n",
        "Der Agent weiß  *nicht*, was das Ziel ist. \n",
        "Vielmehr lernen Sie das Ziel, indem es Belohnungen (oder Strafen) gibt, und der Algorithmus aktualisiert seine Richtlinie zur Wahl von Aktionen so, dass er Aktionen mit größerer Wahrscheinlichkeit erneut durchführt, die voraussichtlich zu einer späteren Belohnung führen. \n",
        "Beachten Sie, dass dies bedeutet, dass ein Agent, wenn er bestimmte Belohnungen nie erhält, nicht weiß, dass sie existieren.\n",
        "\n",
        "- **Vorkenntnisse in Pfadfindung, Physik, etc.:** Als Mensch bringen Sie, auch wenn Sie diese Aufgabe noch nicht gelöst haben, enorme Vorkenntnisse zu diesem Problem mit.\n",
        "Sie wissen beispielsweise, dass der kürzeste Weg zu einem Ziel eine Linie ist.\n",
        "Sie wissen, dass Norden, Süden, Osten und Westen Richtungen sind und dass Sie nach Norden und dann nach Süden zurückkehren, wo Sie bereits waren.\n",
        "Sie wissen, dass Eis rutschig ist.\n",
        "Sie wissen, dass eisiges Wasser kalt ist.\n",
        "Sie wissen, dass es schlecht ist, in eiskaltem Wasser zu sein.\n",
        "Der Agent weiß von all diesen Dingen nichts; seine anfängliche Richtlinie besteht im Wesentlichen darin, Aktionen vollständig zufällig auszuwählen.\n",
        "Am Ende des Trainings wird es immer noch nicht wissen, was abstrakte Konzepte wie \"Nord/Süd\", \"kalt\" oder \"rutschig\" bedeuten, aber es wird (hoffentlich) eine gute Politik gelernt haben, die es ihm ermöglicht, das Ziel zu erreichen.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqtloydzxtpQ",
        "outputId": "2119d6d9-898c-4f04-f4fc-3020e65146db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "print(env.observation_space, env.action_space)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(16) Discrete(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKNMdEtextpS"
      },
      "source": [
        "Über einen `reset() `kann das Environment jeweils \"auf Start\" zurückgesetzt werden. Der Status $s$ wird dann also 0 (das erste Feld) sein.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2MH2fKWxtpS",
        "outputId": "f8e8d906-800c-4c64-da4f-25b1e8163726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "s = env.reset()\n",
        "print(s)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8iWXfrExtpT"
      },
      "source": [
        "Mittels `render()` kann der aktuelle Status dargestellt werden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktoBL6iHxtpT",
        "outputId": "9e455fc1-7df5-4bee-85cb-e50434fef8fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEzQbKCxxtpT"
      },
      "source": [
        "Die Standard FrozenLake-Umgebung ist \"rutschig\". Wegen des Eises haben Sie, wenn Sie versuchen, in eine Richtung zu gehen, eine 1/3-Chance, in die von Ihnen gemeinte Richtung und die beiden angrenzenden Richtungen zu gehen. Wenn wir zum Beispiel versuchen, nach rechts zu gehen, haben wir die gleiche Wahrscheinlichkeit, stattdessen abzurutschen und nach oben und unten zu gehen. Dies macht die Sache etwas komplizierter.\n",
        "\n",
        "Wir definieren uns zunächst eine \"nicht rutschige\" Umgebung, in der die Aktion deterministisch ausgeführt wird.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeIRKfQQxtpU",
        "outputId": "902b328b-52be-4914-baf6-489aca551a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Non-slippery version\n",
        "\n",
        "from gym.envs.registration import register\n",
        "register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        ")\n",
        "env = gym.make('FrozenLakeNotSlippery-v0')\n",
        "env.reset()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-050a8ffe97d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FrozenLakeNotSlippery-v0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gym.envs.toy_text:FrozenLakeEnv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'map_name'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'4x4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_slippery'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FrozenLakeNotSlippery-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot re-register id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Cannot re-register id: FrozenLakeNotSlippery-v0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBmHMsPQxtpU"
      },
      "source": [
        "Mittels `step(action)` führen wir einen Schritt durch.\n",
        "`step(2)` entspricht also einer Bewegung nach rechts.\n",
        "Beachten Sie, dass die Ausgabe ein Tupel von vier Elementen ist: der nächste Statusg (`object`), der reward (`float`), ob die Episode fertig ist (`boolean`) und weitere Infos (`dict` ), die für das Debuggen nützlich sein können."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xux3PZInxtpV",
        "outputId": "443aeab8-3a72-4c09-db35-9a9b9d100c1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.reset()\n",
        "env.render()\n",
        "env.step(2)\n",
        "env.render()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TKbXhkuxtpW"
      },
      "source": [
        "Next, let's `render()` to visualize what happened.\n",
        "Observe that this particular environment prints out the action we took in parentheses up top, in this case \"(Right)\", and then shows the result of that action.\n",
        "Notice that while most of the time, we succeed in going in the direction we want to, occasionally we slip on the ice and go in a direction we didn't intend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN7Ua_KvxtpW",
        "outputId": "89990a6b-68cd-4401-9628-c73ce3d9aff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COXHdmzgxtpX"
      },
      "source": [
        "We can keep doing this as many times as we want.\n",
        "Since we're in Jupyter, we can just keep running the same cell (making small edits to change our action).\n",
        "\n",
        "Notice that once we fall into a hole, the episode is over, and we can no longer do anything.\n",
        "The same is true after reaching the goal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eIFgs4FuxtpX",
        "outputId": "a73ae27c-aeeb-4cea-e4ca-5e034ae8d58d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.step(0)\n",
        "env.render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSediE4_xtpX"
      },
      "source": [
        "Before we get into any RL, let's see how random actions perform in this environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wbEHr1RxtpX",
        "outputId": "de349e48-7400-42db-9aa1-16b17dc799ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = env.action_space.sample()\n",
        "    _, _, done, _ = env.step(action)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Up)\n",
            "SFF\u001b[41mF\u001b[0m\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fcbe6FuxtpY"
      },
      "source": [
        "Hm. \n",
        "Not great. \n",
        "Alright, so clearly picking random steps isn't very likely to take us to the goal.\n",
        "It's apparent just from looking at the map that there're much better policies that we can learn.\n",
        "How are we going to do so?\n",
        "\n",
        "#### Q-learning\n",
        "There are many algorithms that we can use, but let's choose Q-learning, which we covered earlier today.\n",
        "Remember, in Q-learning (and SARSA, it turns out), we're trying learn the Q values for the states in our system.\n",
        "\n",
        "The Q value for a policy $\\pi$ is a function of the state $s$ and action $a$ and is defined as the following:\n",
        "\\begin{equation}\n",
        "Q_\\pi (s,a) = \\mathbb{E}\\big[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\big|\\pi, s_0 = s, a_0 = a\\big]\n",
        "\\end{equation}\n",
        "Intuitively, the Q value is the total reward (including discounting) that the agent will gain if it takes action $a$ from state $s$ and then follows policy $\\pi$ for the rest of the episode.\n",
        "As one might expect, if Q is known exactly, the agent will attain the highest reward from $s$ if the policy $\\pi$ is to pick the $a$ with the highest Q value.\n",
        "\n",
        "Okay, so if we know the Q values for the system, then we can trivially find the optimal policy.\n",
        "So what are the Q values of the system?\n",
        "Well, at the beginning, we don't know, but we can try to learn them through experience.\n",
        "This is where Q-learning comes in.\n",
        "Q-learning iteratively updates the Q values in the following way:\n",
        "\\begin{equation}\n",
        "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma \\max_a Q_\\pi(s_{t+1}, a)\\big)\n",
        "\\end{equation}\n",
        "Notice that Q-learning is an *off-policy* method, in the sense that you don't actually learn from the trajectory you actually took (otherwise it'd be SARSA).\n",
        "Instead, we learn from the *greedy* transition, i.e. the best action we know how to take.\n",
        "\n",
        "And that's it! \n",
        "We run our agent through many episodes, experiencing many $s_t \\rightarrow a_t \\rightarrow s_{t+1}$ transitions and rewards, and just like that, we eventually learn a good Q function (and thus a good policy).\n",
        "Now of course, there are a bunch of small details and tweaks to make this work in practice, but we'll get to those later.\n",
        "\n",
        "#### Q-learning in FrozenLake\n",
        "FrozenLake is a very simple setting, one that we would call a toy problem.\n",
        "With only 16 states and 4 actions, there are only 64 state-action pairs possible (16x4=64), less if we account for the goal and the holes being episode ending (for simplicity though, we won't). \n",
        "With this few state-action pairs, we can actually solve this problem tabularly.\n",
        "Let's set up a Q table, and initialize the Q-values for all state-action pairs to zeros.\n",
        "Note that while we could, we're actually not going to need PyTorch in this example; PyTorch's autograd and neural network libraries are unnecessary here, as we're only going to be modifying a table of numbers.\n",
        "Instead, we'll use a Numpy array to store the Q table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAuxrXQoxtpY"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#Initialize table with all zeros to be uniform\n",
        "Q = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsLlVkBgxtpY"
      },
      "source": [
        "A few hyperparameters we're going to set:\n",
        "- `alpha`: learning rate for the Q function\n",
        "- `gamma`: discount rate for future rewards\n",
        "- `num_episodes`: number of episodes (trajectories from start to goal/hole) our agent will learn from\n",
        "\n",
        "We're also going to store our rewards in an array called `rs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9MRVzCLxtpZ"
      },
      "source": [
        "# Learning parameters\n",
        "alpha = 0.1\n",
        "gamma = 0.95\n",
        "num_episodes = 2000\n",
        "\n",
        "# array of reward for each episode\n",
        "rs = np.zeros([num_episodes])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95kZBLorxtpZ"
      },
      "source": [
        "Now for the bulk of algorithm itself.\n",
        "Notice that we're going to loop through the process `num_episodes` times, resetting the environment each time.\n",
        "At each step, we take the action with the highest Q value for our current state, with some randomness added in (especially at the beginning) to encourage exploration.\n",
        "After each action, we update our Q table greedily based on the reward experienced and the next best action.\n",
        "We also make sure to update our state, rinse, and repeat. \n",
        "We continue taking actions in an episode until it is `done`, storing the final total reward for the episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGrTTDxnxtpZ"
      },
      "source": [
        "for i in range(num_episodes):\n",
        "    # Set total reward and time to zero, done to False\n",
        "    r_sum_i = 0\n",
        "    t = 0\n",
        "    done = False\n",
        "    \n",
        "    #Reset environment and get first new observation\n",
        "    s = env.reset()\n",
        "    \n",
        "    while not done:\n",
        "        # Choose an action by greedily (with noise) from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i/10+1)))\n",
        "        \n",
        "        # Get new state and reward from environment\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        \n",
        "        # Update Q-Table with new knowledge\n",
        "        Q[s,a] = (1 - alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:]))\n",
        "        \n",
        "        # Add reward to episode total\n",
        "        r_sum_i += r*gamma**t\n",
        "        \n",
        "        # Update state and time\n",
        "        s = s1\n",
        "        t += 1\n",
        "    rs[i] = r_sum_i"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGdEKkVoxtpZ"
      },
      "source": [
        "How did we do?\n",
        "Let's take a look at the rewards that we saved.\n",
        "We can plot the reward versus the episode number, and hopefully we'll see some sort of an increase over time.\n",
        "RL performance can be extremely noisy, so let's instead plot a moving average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbopkBISxtpZ",
        "outputId": "3ac4c37b-8ec2-4a2c-bbdc-a60d9bdfaeb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "## Plot reward vs episodes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sliding window average\n",
        "r_cumsum = np.cumsum(np.insert(rs, 0, 0)) \n",
        "r_cumsum = (r_cumsum[50:] - r_cumsum[:-50]) / 50\n",
        "\n",
        "# Plot\n",
        "plt.plot(r_cumsum)\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s7OIdt3xtpa"
      },
      "source": [
        "Pretty good.\n",
        "We might also be interested in how often our agent actually reached the goal.\n",
        "This won't account for how quickly the agent got there (which might also of interest), but let's ignore that for now.\n",
        "To prevent us from being overwhelmed by data points, let's bucket the values into 10 stages, printing out how many episodes of each stage resulted in finding the goal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nntSJxAHxtpa",
        "outputId": "0c22fbe9-7d45-44cc-fd12-a5269e554e40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print number of times the goal was reached\n",
        "N = len(rs)//10\n",
        "num_Gs = np.zeros(10)\n",
        "\n",
        "for i in range(10):\n",
        "    num_Gs[i] = np.sum(rs[i*N:(i+1)*N] > 0)\n",
        "    \n",
        "print(\"Rewards: {0}\".format(num_Gs))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rewards: [  9. 164. 200. 200. 200. 200. 200. 200. 200. 200.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAko4mdhxtpa"
      },
      "source": [
        "Our RL agent does a really good job at navigating the FrozenLake when its moves are deterministic, but after all, this is\n",
        "supposed to be *Frozen*Lake, so where's the fun without the slipperiness?\n",
        "Let's go back to the original environment and see how the agent does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6UrQZwP8iVu",
        "outputId": "654c0e9e-51a9-44a1-adaa-ffe2591f1ca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c4znjcP4cLZ"
      },
      "source": [
        "def perform_and_render():\n",
        "  \n",
        "  s = env.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    print(env.nS)\n",
        "    env.render()\n",
        "    action = np.argmax(Q[s,:]) \n",
        "    print(action)\n",
        "    s, _, done, _ = env.step(action)\n",
        "  env.render()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkEUXxTy5LqE",
        "outputId": "d4c423bd-dd84-4f00-8428-74d9cbabd8ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "perform_and_render()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "1\n",
            "0\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "1\n",
            "0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "2\n",
            "0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "2\n",
            "0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "1\n",
            "0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "2\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciSJHTd9xtpa",
        "outputId": "bb55799f-2463-449b-c67e-672cb45495f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "#Initialize table with all zeros to be uniform\n",
        "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Learning parameters\n",
        "alpha = 0.1\n",
        "gamma = 0.95\n",
        "num_episodes = 2000\n",
        "\n",
        "# array of reward for each episode\n",
        "rs = np.zeros([num_episodes])\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    # Set total reward and time to zero, done to False\n",
        "    r_sum_i = 0\n",
        "    t = 0\n",
        "    done = False\n",
        "    \n",
        "    #Reset environment and get first new observation\n",
        "    s = env.reset()\n",
        "    \n",
        "    while not done:\n",
        "        # Choose an action by greedily (with noise) from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i/10+1)))\n",
        "        \n",
        "        # Get new state and reward from environment\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        \n",
        "        # Update Q-Table with new knowledge\n",
        "        Q[s,a] = (1 - alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s1,:]))\n",
        "        \n",
        "        # Add reward to episode total\n",
        "        r_sum_i += r*gamma**t\n",
        "        \n",
        "        # Update state and time\n",
        "        s = s1\n",
        "        t += 1\n",
        "    rs[i] = r_sum_i\n",
        "\n",
        "## Plot reward vs episodes\n",
        "# Sliding window average\n",
        "r_cumsum = np.cumsum(np.insert(rs, 0, 0)) \n",
        "r_cumsum = (r_cumsum[50:] - r_cumsum[:-50]) / 50\n",
        "\n",
        "# Plot\n",
        "plt.plot(r_cumsum)\n",
        "plt.show()\n",
        "\n",
        "# Print number of times the goal was reached\n",
        "N = len(rs)//10\n",
        "num_Gs = np.zeros(10)\n",
        "\n",
        "for i in range(10):\n",
        "    num_Gs[i] = np.sum(rs[i*N:(i+1)*N] > 0)\n",
        "    \n",
        "print(\"Rewards: {0}\".format(num_Gs))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Rewards: [ 2.  5.  9. 16. 22. 24. 26. 51. 40. 28.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5V-Qq-6AbDo",
        "outputId": "3462ade6-49df-4d88-f6b6-d005d2f5fe47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "perform_and_render()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "16\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "16\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "3\n",
            "16\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "3\n",
            "16\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "16\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "16\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "16\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "16\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "3\n",
            "16\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "16\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "3\n",
            "16\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "3\n",
            "16\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "3\n",
            "16\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "3\n",
            "16\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "0\n",
            "16\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "3\n",
            "16\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "2\n",
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1nmNiY0xtpb"
      },
      "source": [
        "Much harder.\n",
        "However, we can see that the model does eventually learn something."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEt834ZPxtpb"
      },
      "source": [
        "### PyTorch in RL\n",
        "Hey, not bad.\n",
        "However, while the previous example was fun and simple, it was noticeably lacking any hint of PyTorch.\n",
        "\n",
        "We could have used a PyTorch `Tensor` to store the Q table, but that's not any better than using a NumPy array.\n",
        "PyTorch's true utility comes from building neural networks and calculating/applying gradients automatically, which learning the Q table didn't need. \n",
        "\n",
        "#### Continuous domains\n",
        "In our previous example, we mentioned that with only 16 discrete states and 4 actions/state, the Q table only needed to hold 64 values, which is very manageable.\n",
        "However, what if the state or action space is continuous?\n",
        "You could discretize it, but then you have to pick a resolution, and your state-action space could explode exponentially.\n",
        "Treating these binned states or actions as completely different states is also ignoring that two consecutive bins are likely very similar in the needed policy.\n",
        "You can learn these relationships, but doing so is horriby sample inefficient.\n",
        "\n",
        "Instead of learning a Q table then, perhaps a Q function would be more appropriate.\n",
        "This function would take in a state and action as an input and return a Q value as an output.\n",
        "The Q function may be very complex, but as we've learned over the past few days, neural networks are very flexible and good for approximating arbitrary functions.\n",
        "[Deep Q Networks](https://deepmind.com/research/dqn/) take such an approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-2wjWr-xtpb"
      },
      "source": [
        "### Cart Pole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGHESFQxxtpb"
      },
      "source": [
        "Let's look at the cart pole problem next. \n",
        "In this setting, we have a pole attached to a hinge on a cart, with the goal being to keep the pole vertical as long as possible, without traveling too far along the rail.\n",
        "Because of gravity, the pole will fall unless the cart is exactly beneath the the pole's center of gravity.\n",
        "To prevent the pole from falling, the agent can apply a force of +1 or -1 to the cart to move it left and right along a track.\n",
        "The agent receives a reward of +1 for every timestamp the pole remains vertical; the game ends when the pole fall past 15 degrees from vertical or the cart moves more than 2.4 units away from the center.\n",
        "We're going to somewhat arbitrarily call \"success\" achieving a reward of +200; alternatively, the agent needs to avoid the aforementioned failure conditions for 200 ticks.\n",
        "\n",
        "<img src=\"https://github.com/dnhshl/cc-ai/blob/main/Figures/polebalance.gif?raw=1\" alt=\"polebalance\" style=\"width: 400px;\"/>\n",
        "\n",
        "First, let's create an instance of the cart pole environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-2WOBHUxtpb"
      },
      "source": [
        "env = gym.make('CartPole-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDVqRBD2xtpb"
      },
      "source": [
        "Again, we can look at the `observation_space` for this environment.\n",
        "Also similar to FrozenLake, since this version of cart pole is an MDP (as opposed to POMDP), the observation is the state itself.\n",
        "We can see that the states for cart pole have 4 dimensions, which correspond to `[cart position, cart velocity, pole angle, pole angular velocity]`.\n",
        "Importantly, notice these states are *continuous* values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HilweWrExtpc",
        "outputId": "62266988-ed9a-435d-a4db-ed6c4b482ff8"
      },
      "source": [
        "env.observation_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf0qx_wyxtpc"
      },
      "source": [
        "We can look at the `action_space` again too.\n",
        "In cart pole, there are two actions available to the agent: `[apply force left, apply force right]`.\n",
        "We can see this by examining the `action_space` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK6-CIuCxtpc",
        "outputId": "a2590e16-fc64-43ef-8538-9e8a207bfd36"
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf_mGgq7xtpc"
      },
      "source": [
        "Resetting the environment returns our first observations, which we can see has 4 values corresponding to the 4 previously mentioned state variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfeO-5ZOxtpc",
        "outputId": "ef1ad667-ead1-4767-c777-b47733663b49"
      },
      "source": [
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0239603 , -0.03830238,  0.03206419,  0.03205805])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aArS6kYmxtpd"
      },
      "source": [
        "Before we get into any reinforcement learning, let's see how we perform actions within the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QAV6wpvIxtpd"
      },
      "source": [
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = env.action_space.sample()\n",
        "    _, _, done, _ = env.step(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g8aMzYextpd"
      },
      "source": [
        "Okay, so clearly choosing a random action at every time step doesn't really achieve our goal of keeping the pole vertical.\n",
        "We're going to need something smarter.\n",
        "\n",
        "Let's close that rendering window.\n",
        "We do this with `close()`.\n",
        "Note that `gym` renderings can be a little finicky, especially on Windows; either `close()` or restarting your Jupyter kernel may be necessary to close the rendered window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yolzv8_xtpd"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dHUKLaSxtpd"
      },
      "source": [
        "Cart pole is actually a fairly simple problem (it's very low dimensional), and so there are simpler ways to do this, but since we've been having so much fun with deep learning, let's use a neural network.\n",
        "Specifically, let's build a DQN that uses Q-learning to learn how to balance the pole.\n",
        "We're going to give our DQN agent 1000 episodes to try and reach the goal of 200 ticks.\n",
        "\n",
        "There are a lot of small details that go into making these models work well, so instead of going through it piece by piece, the full code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "s0g_kQz9xtpd",
        "outputId": "e39243dc-1e22-4170-e157-34de323bf804"
      },
      "source": [
        "# Based on: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "import math\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(4, 24)\n",
        "        self.fc2 = nn.Linear(24, 48)\n",
        "        self.fc3 = nn.Linear(48, 2)\n",
        "\n",
        "    def forward(self, x):        \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x        \n",
        "    \n",
        "\n",
        "class DQNCartPoleSolver:\n",
        "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_log_decay\n",
        "        self.alpha = alpha\n",
        "        self.alpha_decay = alpha_decay\n",
        "        self.n_episodes = n_episodes\n",
        "        self.n_win_ticks = n_win_ticks\n",
        "        self.batch_size = batch_size\n",
        "        self.quiet = quiet\n",
        "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
        "\n",
        "        # Init model\n",
        "        self.dqn = DQN()\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.opt = torch.optim.Adam(self.dqn.parameters(), lr=0.01)\n",
        "\n",
        "    def get_epsilon(self, t):\n",
        "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        return torch.tensor(np.reshape(state, [1, 4]), dtype=torch.float32) \n",
        "    \n",
        "    def choose_action(self, state, epsilon):\n",
        "        if (np.random.random() <= epsilon):\n",
        "            return self.env.action_space.sample() \n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return torch.argmax(self.dqn(state)).numpy()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        reward = torch.tensor(reward)\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def replay(self, batch_size):\n",
        "        y_batch, y_target_batch = [], []\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            y = self.dqn(state)\n",
        "            y_target = y.clone().detach()\n",
        "            with torch.no_grad():\n",
        "                y_target[0][action] = reward if done else reward + self.gamma * torch.max(self.dqn(next_state)[0])\n",
        "            y_batch.append(y[0])\n",
        "            y_target_batch.append(y_target[0])\n",
        "        \n",
        "        y_batch = torch.cat(y_batch)\n",
        "        y_target_batch = torch.cat(y_target_batch)\n",
        "        \n",
        "        self.opt.zero_grad()\n",
        "        loss = self.criterion(y_batch, y_target_batch)\n",
        "        loss.backward()\n",
        "        self.opt.step()        \n",
        "        \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def run(self):\n",
        "        scores = deque(maxlen=100)\n",
        "\n",
        "        for e in range(self.n_episodes):\n",
        "            state = self.preprocess_state(self.env.reset())\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                if e % 100 == 0 and not self.quiet:\n",
        "                    self.env.render()\n",
        "                action = self.choose_action(state, self.get_epsilon(e))\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = self.preprocess_state(next_state)\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "\n",
        "            scores.append(i)\n",
        "            mean_score = np.mean(scores)\n",
        "            if mean_score >= self.n_win_ticks and e >= 100:\n",
        "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
        "                return e - 100\n",
        "            if e % 100 == 0 and not self.quiet:\n",
        "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
        "\n",
        "            self.replay(self.batch_size)\n",
        "        \n",
        "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
        "        return e\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    agent = DQNCartPoleSolver()\n",
        "    agent.run()\n",
        "    agent.env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 100 episodes was 14.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 19.75 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 58.94 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 94.49 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 117.56 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 190.01 ticks.\n",
            "Ran 504 episodes. Solved after 404 trials ✔\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_puaZFYTxtpe"
      },
      "source": [
        "Reinforcement learning can be kind of noisy.\n",
        "In some sense, it depends on your agent \"lucking\" into the right behavior so that it can learn from it, and occasionally one can get stuck in a bad rut.\n",
        "Even if your agent fails to \"solve\" the problem (i.e. reach 200 ticks), you should still see the mean survival time mostly climbing as the agent experiences more episodes.\n",
        "You may need to re-run learning a couple times for the agent to reach 200 ticks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94FPDYVTxtpe"
      },
      "source": [
        "Want to take this farther and connect this with the convolutional neural networks we've previously learned about?\n",
        "See [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) for a version of Cartpole where instead of directly measuring the translation/rotational position/velocity, the agent only sees the rendered screen.\n",
        "This makes the problem much harder, as the agent needs to learn to infer translation/rotational position/velocity from images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTw7KZ_hxtpe"
      },
      "source": [
        "### Other materials:\n",
        "Reinforcement Learning can easily be a full course (or multiple) on its own at most universities, with or without deep learning.\n",
        "We only touch on a couple points here.\n",
        "If you're interested in *exploring* this field further, a few recommendations to get you started:\n",
        "\n",
        "- [The definitive textbook on Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)\n",
        "- [Video lectures by David Silver](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n",
        "- [Deep Q-Networks (DQN) for Atari 2600 games](https://deepmind.com/research/dqn/) \n",
        "- [Popular blog post on using policy gradients to learn to play Pong from pixels](http://karpathy.github.io/2016/05/31/rl/)"
      ]
    }
  ]
}